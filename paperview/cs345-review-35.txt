

==+== Review Readiness
==-== Enter "Ready" if the review is ready for others to see:

Ready

==+== A. Overall merit
==-== Choices:
==-==    1. Reject
==-==    2. Weak reject
==-==    3. Weak accept
==-==    4. Accept
==-==    5. Strong accept
==-== Enter the number of your choice:

5

==+== B. Paper summary


large models can be quite difficult to train due to memory constraints. This paper proposes a intra-layer tensor parallel approach that enables training transformer models with billions of parameters.



==+== C. Strengths
==-==    What are the paper’s strengths? Just a couple sentences, please.

1. It can train an 8.3 billion parameter transformer language model to achieve SOTA results.
2. It achieves 76% scaling efficiency with 512 GPUs.
3. It proposes a dedicated parallelism way to reduce communication cost.



==+== D. Weaknesses
==-==    What are the paper’s weaknesses? Just a couple sentences,
==-==    please.

1. Training a model with more than 16 billion parameters will demand more memory than is available within 16 GPUs of a DGX-2H box. The ZeRO paper solves this problem.


==+== E. Detailed comments

Strengths
1. Before this paper, tensor parallel requires new frameworks or copmilers. This paper don't require any new compiler.
2. This paper splits A along its columns can remove a synchronization point. Similarly, it splits K,Q,V in a column parallel fashion.
3. In SC'21, Nividia uses megatron-LM to train 1 trillion parameters model at 502 petaFLOP/s on 3072 GPUs. In NSDI'24, Bytedance use 10000 GPUs, MegaScale is built on top of Megatron-LM. proves its success in future. 

Weaknesses
1. The input and output (B, S, H) of each model block need to be repeatedly stored multiple times.


==+== Scratchpad (for unsaved private notes)

==+== End Review
