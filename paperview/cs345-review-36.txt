==+== CS345 Paper Review Form
==-== DO NOT CHANGE LINES THAT START WITH "==+==" UNLESS DIRECTED!
==-== For further guidance, or to upload this file when you are done, go to:
==-== https://hotcrp.kaust.edu.sa/cs345/offline

==+== =====================================================================
==+== Begin Review #36.
==+== Reviewer: Juyi Lin <LINJ>

==+== Paper #36
==-== Title: ZeRO: Memory Optimizations Toward Training Trillion Parameter
==-==        Models

==+== Review Readiness
==-== Enter "Ready" if the review is ready for others to see:

Ready

==+== A. Overall merit
==-== Choices:
==-==    1. Reject
==-==    2. Weak reject
==-==    3. Weak accept
==-==    4. Accept
==-==    5. Strong accept
==-== Enter the number of your choice:

5

==+== B. Paper summary

Large models has billions to trillions of parameters, existing solutions cannot fit them into limited GPU memory. ZeRO can reduce memory consumption to train larger models. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency.



==+== C. Strengths
==-==    What are the paper’s strengths? Just a couple sentences, please.

1. ZeRO has good performance and can scale to large model size. 
2. ZeRO has good usability. ZeRO can train large models of up to 13B parameters without requiring model parallelism which is harder for scientists to apply.
3. ZeRO can train the largest language model at the time with SOTA accuracy.



==+== D. Weaknesses
==-==    What are the paper’s weaknesses? Just a couple sentences,
==-==    please.



==+== E. Detailed comments



Methods
1. ZeRO-DP parition Optimizer State, gradient and parameter to save memory consumption in each GPU. 
2. The authors develop ZeRO-R to optimize the residual memory consumed by these three factors respectively. ZeRO-R removes activation replication in MP through activation partitioning. It also offloads activations to CPU when appropriate.
3. ZeRO-R create a fixed buffer to bucket multiple group of small data. 
4. ZeRO-R proactively manages memory based on the different lifetime of tensors, preventing memory fragmentation.
5. ZeRO-R preallocate memory for activation checkpoint and gradient to prevent memory fragmentation.


Strengths
1. This paper implement and evaluate ZeRO-100B — Pos+g of ZeRO-DP plus ZeRO-R —, that represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art.
2. This paper train Turing-NLG, the largest language model at the time (17B parameters) with record breaking accuracy.
3. This paper have released all implementations described in this paper.


==+== Scratchpad (for unsaved private notes)

==+== End Review
