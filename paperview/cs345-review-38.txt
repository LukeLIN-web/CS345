==+== CS345 Paper Review Form
==-== DO NOT CHANGE LINES THAT START WITH "==+==" UNLESS DIRECTED!
==-== For further guidance, or to upload this file when you are done, go to:
==-== https://hotcrp.kaust.edu.sa/cs345/offline

==+== =====================================================================
==+== Begin Review #38.
==+== Reviewer: Juyi Lin <LINJ>

==+== Paper #38
==-== Title: ARK: GPU-driven Code Execution for Distributed Deep Learning

==+== Review Readiness
==-== Enter "Ready" if the review is ready for others to see:

Ready

==+== A. Overall merit
==-== Choices:
==-==    1. Reject
==-==    2. Weak reject
==-==    3. Weak accept
==-==    4. Accept
==-==    5. Strong accept
==-== Enter the number of your choice:

4

==+== B. Paper summary

The collective communication overhead across GPUs harm the distributed deep learning performance. This paper propose a GPU-driven code execution system that leverages a GPU-controlled hardware DMA engine for I/O offloading. GPU threads directly control DMA operations. GPUs drive their own execution flow and handle communication events autonomously without CPU intervention. ARK achieves a line-rate from a pretty small message with very low communication latency while it incurs little interference with computation on GPU, achieving 1.8x higher all-reduce throughput.


==+== C. Strengths
==-==    What are the paper’s strengths? Just a couple sentences, please.

1. ARK can achieve better throughput in different parallelism and different scanrio(training and inference)
2. ARK customs a GPU-controlled DMA engine to reduce heavy MMIO operations.
3. ARK can achieve low transfer delay for small chunks.

==+== D. Weaknesses
==-==    What are the paper’s weaknesses? Just a couple sentences,
==-==    please.



==+== E. Detailed comments

Strengths
1. ARK supports data-, tensor-, and pipeline-parallelisms. ARK delivers substantial performance gains both in training and inference, achieving 2.5x and 3.6x throughput improvement, respectively.
2. ARK designs a GPU-controlled DMA engine to save I/O overhead on GPU. ARK custom DMA engine is directly initiated by GPU threads, which avoids the heavy MMIO without CPU intervention.
3. ARK proposes a new idea beyond operator fusion. It schedules all operators in a single loop kernel, similar as operator fusion.


Weaknesses
1. This paper doesn't evaluate Megatron-LM in multiple nodes setting, While it doesn't mention whether SuperBench(which it uses for multi-node experiments) is better than Megatron-LM in that case.






==+== Scratchpad (for unsaved private notes)

==+== End Review
